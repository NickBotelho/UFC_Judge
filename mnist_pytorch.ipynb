{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7.1\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "import torch.nn as NN\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model #CNN Model\n",
    "class Net(NN.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__() #initialize the superclass\n",
    "         #2 convolutionaal layers\n",
    "        self.conv1 = NN.Conv2d(1, 20, 5, stride=1) #1 layer\n",
    "        self.conv2 = NN.Conv2d(20, 50, 5, stride=1) #output of first layer will be input to next layer, 20 convolutional layers and the output will be 50 conv layers\n",
    "        #20 and 50 are 'hyperparameters' and these are somewhat arbitrary based on performance\n",
    "        self.pool = NN.MaxPool2d(2, 2)\n",
    "        self.fc1 = NN.Linear(4*4*50, 20) #fully connected network, output will be 20 dimensions\n",
    "        self.fc2 = NN.Linear(20, 10)\n",
    "\n",
    "    def forward(self, x): #the whole pipeline of the computation\n",
    "        x = self.pool( F.relu(self.conv1(x)) ) #run through the first conv layers then apply the activation function\n",
    "        #ReLU takes input, anything below zero it returns zero\n",
    "        #No padding here, pooling reduces size of images by half\n",
    "        x = self.pool( F.relu(self.conv2(x)) ) #apply second conv layer with relu and pooling\n",
    "        x = x.view(-1, 4*4*50) #flattens the image\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "        \n",
    "def train(model, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad() #for each new data make a new gradient\n",
    "        #output = model(torch.autograd.Variable(data))\n",
    "        output = model( data )\n",
    "        loss = F.nll_loss(output, target) #gets the error between the model output and the ground truth\n",
    "        #.nll_loss is an error function\n",
    "        loss.backward() #backpropogate the gradient\n",
    "        optimizer.step() #update the parameters \n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch+1, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "def test(model, test_loader): #test the model on the test dataset\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91e7c22d137742f3a3c3eb08904a8e0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ../data\\MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f003085beec74082b8fa136ac91452b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ../data\\MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22199f4dc1c94e7e911512242704b616",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ../data\\MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53f4753a45824e5e927baaf8c99fd624",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ../data\\MNIST\\raw\n",
      "Processing...\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torchvision\\datasets\\mnist.py:480: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:141.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "train_batchsize = 64\n",
    "test_batchsize = 1000\n",
    "trainEpochs = 10\n",
    "learningRate = 0.01\n",
    "sgdmomentum = 0.9\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "#tranform converts input into tensoors aand then normalize it\n",
    "\n",
    "trainset = datasets.MNIST(root='../data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=train_batchsize, shuffle=True)\n",
    "\n",
    "testset = datasets.MNIST(root='../data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=test_batchsize, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 1, 28, 28])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check samples I\n",
    "examples = enumerate(testloader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)\n",
    "example_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check samples II\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "for i in range(6):\n",
    "  plt.subplot(2,3,i+1)\n",
    "  plt.tight_layout()\n",
    "  plt.imshow(example_data[i][0].numpy(), cmap='gray', interpolation='none')\n",
    "  plt.title(\"Ground Truth: {}\".format(example_targets[i]))\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py:130: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at  ..\\c10\\cuda\\CUDAFunctions.cpp:100.)\n",
      "  Variable._execution_engine.run_backward(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.284337\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.343673\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.366328\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.109828\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.110291\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.049436\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.141679\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.062940\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.043758\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.132893\n",
      "\n",
      "Test set: Average loss: 0.0492, Accuracy: 9853/10000 (99%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.093009\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.058254\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.092242\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.071805\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.057262\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.065759\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.040552\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.082740\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.037248\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.035391\n",
      "\n",
      "Test set: Average loss: 0.0374, Accuracy: 9879/10000 (99%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.033233\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.070824\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.001182\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.065335\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.008851\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.002369\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.041505\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.014862\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.035499\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.025851\n",
      "\n",
      "Test set: Average loss: 0.0388, Accuracy: 9864/10000 (99%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.051603\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.012062\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.017520\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.055991\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.029915\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.021964\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.002997\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.047902\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.001039\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.001647\n",
      "\n",
      "Test set: Average loss: 0.0371, Accuracy: 9876/10000 (99%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.007594\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.001566\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.015904\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.019781\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.008864\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.046564\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.021834\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.028281\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.007856\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.288417\n",
      "\n",
      "Test set: Average loss: 0.0280, Accuracy: 9904/10000 (99%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.004033\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.004173\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.034333\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.002699\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.000456\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.019654\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.004062\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.000491\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.002697\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.038373\n",
      "\n",
      "Test set: Average loss: 0.0295, Accuracy: 9915/10000 (99%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.012444\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.024540\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.017572\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.006194\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.022583\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.029267\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.000274\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.000101\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.005521\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.002336\n",
      "\n",
      "Test set: Average loss: 0.0306, Accuracy: 9909/10000 (99%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.046015\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.006452\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.019556\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.030823\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.100105\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.060014\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.012035\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.002369\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.014896\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.006156\n",
      "\n",
      "Test set: Average loss: 0.0273, Accuracy: 9923/10000 (99%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.000451\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.003309\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.001054\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.000878\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.007615\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.021057\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.008901\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.014402\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.000307\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.001024\n",
      "\n",
      "Test set: Average loss: 0.0339, Accuracy: 9906/10000 (99%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.024351\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.016609\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.005199\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.000476\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.003887\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.000193\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.003411\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.004254\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.003376\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.003783\n",
      "\n",
      "Test set: Average loss: 0.0302, Accuracy: 9912/10000 (99%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train and test\n",
    "model = Net()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learningRate, momentum = sgdmomentum) #Stochastic Gradient Search\n",
    "\n",
    "#epoch is a minibatch of data\n",
    "for epoch in range(trainEpochs):\n",
    "    train(model, trainloader, optimizer, epoch)\n",
    "    test(model, testloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),\"mnist_cnn.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
